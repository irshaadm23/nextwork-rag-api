ğŸ§  Build a RAG API with FastAPI (Project 1)

This project implements a local Retrieval-Augmented Generation (RAG) API using Python.
The API retrieves relevant information from a custom knowledge base and uses a local AI model to generate accurate, context-aware answers.

The entire system runs locally, without relying on paid cloud APIs or external inference services.

I built this project to understand how APIs, vector databases, and local language models can be composed into a production-style backend system.

ğŸš€ What This Project Does

Exposes a REST API that accepts natural-language questions

Retrieves relevant information from a knowledge base using semantic search

Uses a local AI model to generate answers grounded in retrieved context

Returns responses in structured JSON format

Supports dynamic updates to the knowledge base without restarting the server

ğŸ§© High-Level Architecture

The system follows the standard RAG (Retrieve â†’ Augment â†’ Generate) pattern:

A client sends a question to the API

The API performs semantic search over stored embeddings

Relevant document context is retrieved

The context and question are sent to a local language model

The model generates an answer grounded in the retrieved data

The API returns the response to the client

This design separates retrieval from generation, improving accuracy and reducing hallucination compared to standalone LLM prompts.

ğŸ› ï¸ Key Services & Technologies

FastAPI â€“ API layer that exposes endpoints for querying and updating the knowledge base

Uvicorn â€“ ASGI server that runs the FastAPI application

ChromaDB â€“ Vector database used to store embeddings and perform semantic retrieval

Ollama â€“ Local inference runtime for large language models

TinyLlama â€“ Lightweight local language model used for answer generation

Python Virtual Environment â€“ Isolates project dependencies and Python version

ğŸ“ Project Structure
nextwork-rag-api/
â”œâ”€â”€ README.md        # Project documentation
â”œâ”€â”€ app.py           # FastAPI application
â”œâ”€â”€ embed.py         # Script to embed documents into ChromaDB
â”œâ”€â”€ k8s.txt          # Example knowledge base document
â”œâ”€â”€ db/              # Persistent ChromaDB storage
â”œâ”€â”€ venv/            # Python virtual environment (not committed)
â””â”€â”€ .gitignore
âš™ï¸ Setup & Running the Project
1ï¸âƒ£ Create and activate a virtual environment
py -3.13 -m venv venv
.\venv\Scripts\Activate
2ï¸âƒ£ Install dependencies
pip install fastapi uvicorn chromadb ollama
3ï¸âƒ£ Create embeddings for the knowledge base

Add content to k8s.txt, then run:

python embed.py

This converts the text into embeddings and stores them persistently in ChromaDB.

4ï¸âƒ£ Run the API server
uvicorn app:app --reload

API base URL: http://127.0.0.1:8000

Interactive documentation (Swagger UI): http://127.0.0.1:8000/docs

Hot reloading allows code changes to take effect without manually restarting the server.

ğŸ” Querying the API

Example using PowerShell:

Invoke-RestMethod `
  -Uri "http://127.0.0.1:8000/query?q=What is Kubernetes?" `
  -Method Post

Example response:

{
  "answer": "Kubernetes is a container orchestration platform used to manage containers at scale."
}
â• Adding Knowledge at Runtime

New knowledge can be added while the API is running:

Invoke-RestMethod `
  -Uri "http://127.0.0.1:8000/add?text=Pods are the smallest deployable unit in Kubernetes." `
  -Method Post

The content is embedded immediately and becomes available for subsequent queries without restarting the server.

âš ï¸ Limitations & Design Considerations

This implementation is stateful and intended for local development and learning

Model inference runs in-process via Ollama, which limits concurrency and scalability

The vector store and model runtime are coupled to the API process

In a production environment, these components would typically be:

containerized and deployed as separate services

protected with authentication and request validation

instrumented with logging and monitoring

optimized with document chunking and batching

ğŸ”® Future Improvements

Document chunking for improved retrieval quality

Input validation and schema enforcement

Containerization with Docker

Deployment using Kubernetes

Automated rebuilds when knowledge base data changes

âœ… Project Status

âœ” Completed â€” local RAG API running successfully
âœ” Supports querying and dynamic knowledge updates

ğŸ“ Notes

This project was built using guided steps and then extended to support runtime knowledge updates.
The focus was on understanding system composition, data flow, and operational behavior, rather than model training